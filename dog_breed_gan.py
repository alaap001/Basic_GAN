# -*- coding: utf-8 -*-
"""Dog Breed GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FwHM-1gNpxu9AlOAzX9YugaY_Btso6Pk

## GAN
"""

import torch
import torchvision
import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions
import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.
import torchvision.datasets as datasets  # Has standard datasets we can import in a nice way
import torchvision.transforms as transforms  # Transformations we can perform on our dataset
from torch.utils.data import DataLoader, Dataset # Gives easier dataset managment and creates mini batches
from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard

import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
from PIL import Image

from google.colab import drive
drive.mount('/content/drive')

!unzip drive/My\ Drive/all-dogs.zip

!ls all-dogs/ | head -10

PATH = 'all-dogs/'
images = os.listdir(PATH)

print(f'There are {len(images)} pictures of dogs.')

fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12,10))

for indx, axis in enumerate(axes.flatten()):
    rnd_indx = np.random.randint(0, len(os.listdir(PATH)))
    # https://matplotlib.org/users/image_tutorial.html
    img = plt.imread(PATH + images[rnd_indx])
    imgplot = axis.imshow(img)
    axis.set_title(images[rnd_indx])
    axis.set_axis_off()
plt.tight_layout(rect=[0, 0.03, 1, 0.95])

# Hyperparameters
lr = 0.0005
batch_size = 64
image_size = (225,225)
channels_img = 3
channels_noise = 1024
num_epochs = 75

# For how many channels Generator and Discriminator should use
features_d = 32
features_g = 32

# class Discriminator(nn.Module):
#     def __init__(self, channels_img, features_d):
#         super(Discriminator, self).__init__()
#         self.net = nn.Sequential(
#             nn.Conv2d(channels_img, features_d, 4 , 3 , 1),
#             nn.LeakyReLU(0.2),
            
#             nn.Conv2d(features_d, features_d*2, 4 , 3 , 1),
#             nn.BatchNorm2d(features_d*2),
#             nn.LeakyReLU(0.2),
            
#             nn.Conv2d(features_d*2, features_d*4, 4, 2 , 1),
#             nn.BatchNorm2d(features_d*4),
#             nn.LeakyReLU(0.2),
            
#             nn.Conv2d(features_d*4, features_d*8, 4 , 2 , 0),
#             nn.BatchNorm2d(features_d*8),
#             nn.LeakyReLU(0.2),
            
            
#             nn.Conv2d(features_d*8, 1 , 4 , 2 , 0),
#             nn.Sigmoid()
#         )
        
#     def forward(self,x):
#         return self.net(x)
    
# Discriminator(channels_img, features_d)

class Discriminator(nn.Module):
    def __init__(self, channels_img, features_d):
        super(Discriminator, self).__init__()
        self.net = nn.Sequential(
            nn.Conv2d(channels_img, features_d, 3 , 1 , 1),
            nn.LeakyReLU(0.2),
            
            nn.Conv2d(features_d, features_d*2, 3 , 2 , 1),
            nn.BatchNorm2d(features_d*2),
            nn.LeakyReLU(0.2),
            
            nn.Conv2d(features_d*2, features_d*4, 4 , 2 , 1),
            nn.BatchNorm2d(features_d*4),
            nn.LeakyReLU(0.2),
            
            nn.Conv2d(features_d*4, features_d*8, 4 , 2 , 1),
            nn.BatchNorm2d(features_d*8),
            nn.LeakyReLU(0.2),
            

            nn.Conv2d(features_d*8, features_d*16, 3, 2 , 1),
            nn.BatchNorm2d(features_d*16),
            nn.LeakyReLU(0.2),
            
            nn.Conv2d(features_d*16, features_d*32, 3 , 3 , 1),
            nn.BatchNorm2d(features_d*32),
            nn.LeakyReLU(0.2),
            
            
            nn.Conv2d(features_d*32, 1 , 4 , 2 , 0),
            nn.Sigmoid()
        )
        
    def forward(self,x):
        # for i in self.net(x):
        #   print(x.shape)
        return self.net(x)
    
Discriminator(channels_img, features_d)

# class Generator(nn.Module):
#     def __init__(self, channels_noise, channels_img, features_g):
#         super(Generator,self).__init__()
        
#         self.net = nn.Sequential(
#             nn.ConvTranspose2d(channels_noise, features_g*16, 4, 2, 0),
#             nn.BatchNorm2d(features_g*16),
#             nn.ReLU(),
            
#             nn.ConvTranspose2d(features_g*16, features_g*8, 4, 2, 1),
#             nn.BatchNorm2d(features_g*8),
#             nn.ReLU(),
            
#             nn.ConvTranspose2d(features_g*8, features_g*4, 4, 3, 0),
#             nn.BatchNorm2d(features_g*4),
#             nn.ReLU(),
            
#             nn.ConvTranspose2d(features_g*4, features_g*2, 5, 3, 1),
#             nn.BatchNorm2d(features_g*2),
#             nn.ReLU(),
            
            
#             nn.ConvTranspose2d(features_g*2, channels_img, 5, 3, 1),
#             nn.Tanh()
#         )
#     def forward(self, x):
#         return self.net(x)
    
# Generator(channels_noise, channels_img, features_g)

class Generator(nn.Module):
    def __init__(self, channels_noise, channels_img, features_g):
        super(Generator,self).__init__()
        
        self.net = nn.Sequential(
            nn.ConvTranspose2d(channels_noise, features_g*64, 4, 2, 0),
            nn.BatchNorm2d(features_g*64),
            nn.ReLU(),

            nn.ConvTranspose2d(features_g*64, features_g*32, 4, 2, 1),
            nn.BatchNorm2d(features_g*32),
            nn.ReLU(),

            nn.ConvTranspose2d(features_g*32, features_g*16, 4, 2, 1),
            nn.BatchNorm2d(features_g*16),
            nn.ReLU(),
            
            nn.ConvTranspose2d(features_g*16, features_g*8, 4, 2, 2),
            nn.BatchNorm2d(features_g*8),
            nn.ReLU(),
            
            nn.ConvTranspose2d(features_g*8, features_g*4, 4, 2, 2),
            nn.BatchNorm2d(features_g*4),
            nn.ReLU(),
            
            nn.ConvTranspose2d(features_g*4, features_g*2, 3, 2, 2),
            nn.BatchNorm2d(features_g*2),
            nn.ReLU(),
            
            nn.ConvTranspose2d(features_g*2, channels_img, 3, 2, 1),
            nn.Tanh()
        )
    def forward(self, x):
        return self.net(x)
    
Generator(channels_noise, channels_img, features_g)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using {device}")

# Create discriminator and generator
netD = Discriminator(channels_img, features_d).to(device)
netG = Generator(channels_noise, channels_img, features_g).to(device)

# netD.load_state_dict(torch.load('/content/drive/My Drive/all-dogsD_25.pth')) # load params
# netG.load_state_dict(torch.load('/content/drive/My Drive/all-dogsG_25.pth'))

my_transforms = transforms.Compose(
    [
        transforms.Resize(image_size),
        transforms.CenterCrop(image_size),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ]
)

# Setup Optimizer for G and D
optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))

netG.train()
netD.train()

criterion = nn.BCELoss()

class DogDataset(Dataset):
    def __init__(self, img_dir, transform = None):
        self.img_dir = img_dir
        self.transform = transform
        self.images = os.listdir(img_dir)
        print(f"len of Dataset: {len(images)}")
        
    def __len__(self):
        return len(self.images)
        
    def __getitem__(self, index):
        image = Image.open(os.path.join(self.img_dir, self.images[index]))
        
        if self.transform:
            image = self.transform(image)
            
        return (image,1)

dog_dataset = DogDataset(PATH, my_transforms)
dog_dataset

dataloader = DataLoader(dog_dataset, batch_size = batch_size, shuffle=True)
one_batch = next(iter(dataloader))
one_batch[0].shape

dimg = netD(one_batch[0].to(device))
dimg.shape

noise = torch.randn(batch_size, channels_noise, 1, 1).to(device)
netG(noise).shape

# train_data = datasets.ImageFolder('data/', transform=transform) this is the in-built fn in PyTorch

!mkdir 'generated_images'

from torchvision.utils import save_image
def denorm(x):
    out = (x + 1) / 2
    return out.clamp(0, 1)

def save_fake_images(index):
    noise = torch.randn(64, channels_noise, 1, 1).to(device)
    fake_images = netG(noise)
    fake_images = fake_images.reshape(fake_images.size(0), 3, 225, 225)
    fake_fname = 'generated_images/fake_images-{0:0=4d}.png'.format(index)
    print('Saving', fake_fname)
    save_image(denorm(fake_images), fake_fname, nrow=6)
    
# Before training
save_fake_images(0)
Image.open('fake_images-0000.png')

from torchvision.utils import save_image
def denorm(x):
    out = (x + 1) / 2
    return out.clamp(0, 1)

def save_fake_images(index):
    noise = torch.randn(64, channels_noise, 1, 1).to(device)
    fake_images = netG(noise)
    fake_images = fake_images.reshape(fake_images.size(0), 3, 225, 225)
    fake_fname = 'fake_images-{0:0=4d}.png'.format(index)
    print('Saving', fake_fname)
    save_image(denorm(fake_images), fake_fname, nrow=6)
    
# Before training
save_fake_images(0)
Image.open('fake_images-0000.png')

# train both generator and discriminator
def train(epochs):
    step = 0
    print("Started Training")
    fixed_noise = torch.randn(32, channels_noise, 1, 1).to(device)
    for epoch in range(25,epochs):
        for batch_idx, (image, label) in enumerate(dataloader):
            image = image.to(device)
            batch_size = image.shape[0]
            real_label = (label*0.9).to(device)
            
            ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))
            netD.zero_grad()
            output = netD(image).reshape(-1)
            
            lossD_real = criterion(output, real_label)
            D_x = output.mean().item()
            
            noise = torch.randn(batch_size, channels_noise, 1, 1).to(device)
            fake = netG(noise) # image generated by Generator which is not real
            fake_label = (label * 0.1).to(device)
            
            output = netD(fake.detach()).reshape(-1) # detach as we don't want gradients from this
            
            lossD_fake = criterion(output, fake_label)
            
            lossD_total = lossD_fake + lossD_real
            lossD_total.backward()
            optimizerD.step()
            
            ## Train Generator: max log(D(G(z)))
            netG.zero_grad()
            output = netD(fake).reshape(-1)
            label = torch.ones(batch_size).to(device)
            # if discriminator tells that image is fake, then loss of gen is high
            lossG = criterion(output, label)
            lossG.backward()
            optimizerG.step()
            
            
            # Print losses ocassionally and print to tensorboard
            if batch_idx % 100 == 0:
                step += 1
                print(
                    f"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(dataloader)} \
                      Loss D: {lossD_total:.4f}, loss G: {lossG:.4f} D(x): {D_x:.4f}"
                )
        save_fake_images(epoch)
        if epoch%10==0:
          torch.save(netG.state_dict(), f'drive/My Drive/all-dogsG_{epoch}_2.pth')
          torch.save(netD.state_dict(), f'drive/My Drive/all-dogsD_{epoch}_2.pth')
        # model.load_state_dict(torch.load(PATH)) # load params


#                 with torch.no_grad():
#                     fake = netG(fixed_noise)
#                     img_grid_real = torchvision.utils.make_grid(image[:32], normalize=True)
#                     img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)
#                     writer_real.add_image(
#                         "Mnist Real Images", img_grid_real, global_step=step
#                     )
#                     writer_fake.add_image(
#                         "Mnist Fake Images", img_grid_fake, global_step=step
#                     )
train(num_epochs)

torch.save(netG.state_dict(), 'drive/My Drive/all-dogsG_20.pth')
torch.save(netD.state_dict(), 'drive/My Drive/all-dogsD_20.pth')
# model.load_state_dict(torch.load(PATH)) # load params

2one_batch[0][0].shape

def denorm(x):
    out = (x + 1) / 2
    return out.clamp(0, 1)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
from torchvision.utils import save_image


img_norm =
# trans = transforms.ToPILImage()
save_image( denorm(one_batch[0][0]),"./image_1.jpg")



